import streamlit as st
from transformers import pipeline

# ---------------- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ----------------
@st.cache_resource
def load_model():
    return pipeline(
        "text2text-generation",
        model="google/flan-t5-small",
        device=-1  # CPU ÙÙ‚Ø·
    )

qa_model = load_model()

# ---------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© ----------------
st.set_page_config(page_title="AI Assistant", page_icon="ğŸ¤–", layout="wide")

st.title("ğŸ¤– Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ø®ÙÙŠÙ")
st.caption("Ù…Ø¯Ø¹ÙˆÙ… Ø¨Ù€ FLAN-T5-Small + ONNXRuntime (Ø®ÙŠØ§Ø± Ø§Ù‚ØªØµØ§Ø¯ÙŠ ÙˆØ®ÙÙŠÙ)")

# ---------------- ØªØ¨ÙˆÙŠØ¨Ø§Øª ----------------
tab1, tab2, tab3 = st.tabs(["ğŸ“š Ø³Ø¤Ø§Ù„ ÙˆØ¬ÙˆØ§Ø¨", "ğŸ’¬ Ø¯Ø±Ø¯Ø´Ø©", "ğŸ“ ØªÙ„Ø®ÙŠØµ"])

# =================================================
# --- Ø§Ù„ØªØ§Ø¨ Ø§Ù„Ø£ÙˆÙ„: Ø³Ø¤Ø§Ù„ ÙˆØ¬ÙˆØ§Ø¨ ---
# =================================================
with tab1:
    st.header("â“ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹")
    question = st.text_area("âœï¸ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§")
    if st.button("Ø¥Ø¬Ø§Ø¨Ø©", key="qa"):
        if question.strip():
            result = qa_model(question, max_length=200, do_sample=False)
            st.success(result[0]['generated_text'])
        else:
            st.warning("âš ï¸ Ø§Ù„Ø±Ø¬Ø§Ø¡ ÙƒØªØ§Ø¨Ø© Ø³Ø¤Ø§Ù„ Ø£ÙˆÙ„Ø§Ù‹.")

# =================================================
# --- Ø§Ù„ØªØ§Ø¨ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø¯Ø±Ø¯Ø´Ø© ---
# =================================================
with tab2:
    st.header("ğŸ’¬ Ù…Ø­Ø§Ø¯Ø«Ø© ØªÙØ§Ø¹Ù„ÙŠØ©")

    # ØªÙ‡ÙŠØ¦Ø© Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    chat_input = st.text_area("ğŸ’­ Ø±Ø³Ø§Ù„ØªÙƒ", key="chat_input")

    col1, col2 = st.columns([1, 1])
    with col1:
        send = st.button("Ø¥Ø±Ø³Ø§Ù„", key="chat_send")
    with col2:
        reset = st.button("ğŸ”„ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ†", key="chat_reset")

    # Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„
    if send and chat_input.strip():
        response = qa_model(chat_input, max_length=200, do_sample=False)
        answer = response[0]['generated_text']
        st.session_state.chat_history.append(("ğŸ‘¤", chat_input))
        st.session_state.chat_history.append(("ğŸ¤–", answer))
        st.experimental_rerun()

    # Ø¹Ù†Ø¯ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¹ÙŠÙŠÙ†
    if reset:
        st.session_state.chat_history = []
        st.experimental_rerun()

    # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    if st.session_state.chat_history:
        st.subheader("ğŸ“œ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        for role, msg in st.session_state.chat_history:
            if role == "ğŸ‘¤":
                st.markdown(f"**{role}**: {msg}")
            else:
                st.markdown(f"{role}: {msg}")

# =================================================
# --- Ø§Ù„ØªØ§Ø¨ Ø§Ù„Ø«Ø§Ù„Ø«: ØªÙ„Ø®ÙŠØµ ---
# =================================================
with tab3:
    st.header("ğŸ“ ØªÙ„Ø®ÙŠØµ Ù†ØµÙˆØµ Ø·ÙˆÙŠÙ„Ø©")
    text_input = st.text_area("ğŸ“– Ø¶Ø¹ Ø§Ù„Ù†Øµ Ù‡Ù†Ø§ Ù„Ù„ØªÙ„Ø®ÙŠØµ")
    if st.button("ØªÙ„Ø®ÙŠØµ", key="summ"):
        if text_input.strip():
            summary = qa_model("summarize: " + text_input, max_length=150, do_sample=False)
            st.success(summary[0]['generated_text'])
        else:
            st.warning("âš ï¸ Ø¶Ø¹ Ù†Øµ Ù„Ù„ØªÙ„Ø®ÙŠØµ Ø£ÙˆÙ„Ø§Ù‹.")
